"""
RAG Evaluation Script - –æ—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ RAG —Å–∏—Å—Ç–µ–º –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º ragas
"""
import os
import time
from typing import List, Dict, Any
from dotenv import load_dotenv
from datasets import Dataset
import pandas as pd

from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    context_relevancy
)

from utils import create_vector_store
from utils.pdf_processor import PDFProcessor
from langchain_openai import ChatOpenAI

load_dotenv()


class RAGEvaluator:
    """–ö–ª–∞—Å –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ —è–∫–æ—Å—Ç—ñ RAG —Å–∏—Å—Ç–µ–º"""

    def __init__(self, vector_store_type: str = "chromadb"):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è RAG Evaluator

        Args:
            vector_store_type: –¢–∏–ø –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ö–æ–≤–∏—â–∞ ("chromadb" –∞–±–æ "faiss")
        """
        self.vector_store_type = vector_store_type
        self.vector_store = create_vector_store(vector_store_type)
        self.pdf_processor = PDFProcessor()

        # LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7
        )

    def load_documents(self, pdf_paths: List[str]):
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î PDF –¥–æ–∫—É–º–µ–Ω—Ç–∏ –¥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ö–æ–≤–∏—â–∞

        Args:
            pdf_paths: –°–ø–∏—Å–æ–∫ —à–ª—è—Ö—ñ–≤ –¥–æ PDF —Ñ–∞–π–ª—ñ–≤
        """
        print(f"\nüìÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –¥–æ {self.vector_store_type}...")

        for pdf_path in pdf_paths:
            if not os.path.exists(pdf_path):
                print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {pdf_path}")
                continue

            print(f"   –û–±—Ä–æ–±–∫–∞: {pdf_path}")
            chunks = self.pdf_processor.load_pdf(pdf_path)
            self.vector_store.add_documents(chunks)
            print(f"   ‚úì –î–æ–¥–∞–Ω–æ {len(chunks)} chunk'—ñ–≤")

        total_docs = self.vector_store.get_collection_count()
        print(f"\n‚úì –í—Å—å–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É {self.vector_store_type}: {total_docs}\n")

    def retrieve_context(self, query: str, k: int = 4) -> List[str]:
        """
        –í–∏—Ç—è–≥—É—î —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏ –¥–ª—è –∑–∞–ø–∏—Ç—É

        Args:
            query: –ó–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
            k: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ–≤

        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ–≤
        """
        results = self.vector_store.search(query, k=k)
        contexts = [doc.page_content for doc in results]
        return contexts

    def generate_answer(self, query: str, contexts: List[str]) -> str:
        """
        –ì–µ–Ω–µ—Ä—É—î –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–∞–ø–∏—Ç—É —Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ–≤

        Args:
            query: –ó–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
            contexts: –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏

        Returns:
            str: –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        """
        context_text = "\n\n".join(contexts)

        prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context_text}

–ó–∞–ø–∏—Ç–∞–Ω–Ω—è: {query}

–í—ñ–¥–ø–æ–≤—ñ–¥—å:"""

        response = self.llm.invoke(prompt)
        return response.content

    def evaluate_rag(
        self,
        test_questions: List[Dict[str, Any]],
        metrics: List = None
    ) -> pd.DataFrame:
        """
        –û—Ü—ñ–Ω—é—î —è–∫—ñ—Å—Ç—å RAG —Å–∏—Å—Ç–µ–º–∏ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º ragas

        Args:
            test_questions: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤–∏—Ö –∑–∞–ø–∏—Ç–∞–Ω—å
                –§–æ—Ä–º–∞—Ç: [
                    {
                        "question": "–ø–∏—Ç–∞–Ω–Ω—è",
                        "ground_truth": "–µ—Ç–∞–ª–æ–Ω–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å" (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
                    },
                    ...
                ]
            metrics: –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ ragas –¥–ª—è –æ—Ü—ñ–Ω–∫–∏

        Returns:
            pd.DataFrame: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –æ—Ü—ñ–Ω–∫–∏
        """
        if metrics is None:
            metrics = [
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall,
                context_relevancy
            ]

        print(f"\nüîç –û—Ü—ñ–Ω–∫–∞ RAG —Å–∏—Å—Ç–µ–º–∏ ({self.vector_store_type})...")
        print(f"   –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤–∏—Ö –∑–∞–ø–∏—Ç–∞–Ω—å: {len(test_questions)}")
        print(f"   –ú–µ—Ç—Ä–∏–∫–∏: {[m.name for m in metrics]}\n")

        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –æ—Ü—ñ–Ω–∫–∏
        questions = []
        answers = []
        contexts_list = []
        ground_truths = []

        for i, test_item in enumerate(test_questions, 1):
            question = test_item["question"]
            print(f"   [{i}/{len(test_questions)}] –û–±—Ä–æ–±–∫–∞: {question[:50]}...")

            # –û—Ç—Ä–∏–º—É—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏
            contexts = self.retrieve_context(question, k=4)

            # –ì–µ–Ω–µ—Ä—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
            answer = self.generate_answer(question, contexts)

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏
            questions.append(question)
            answers.append(answer)
            contexts_list.append(contexts)

            # Ground truth (—è–∫—â–æ —î)
            if "ground_truth" in test_item:
                ground_truths.append(test_item["ground_truth"])
            else:
                ground_truths.append(answer)  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å

        # –°—Ç–≤–æ—Ä—é—î–º–æ dataset –¥–ª—è ragas
        eval_dataset = Dataset.from_dict({
            "question": questions,
            "answer": answers,
            "contexts": contexts_list,
            "ground_truth": ground_truths
        })

        # –í–∏–∫–æ–Ω—É—î–º–æ –æ—Ü—ñ–Ω–∫—É
        print("\n   –í–∏–∫–æ–Ω–∞–Ω–Ω—è –æ—Ü—ñ–Ω–∫–∏ ragas...")
        start_time = time.time()

        result = evaluate(
            eval_dataset,
            metrics=metrics,
            llm=self.llm,
            embeddings=self.vector_store.embeddings
        )

        elapsed_time = time.time() - start_time

        print(f"   ‚úì –û—Ü—ñ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥\n")

        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤ DataFrame
        results_df = result.to_pandas()

        # –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ vector store
        results_df['vector_store'] = self.vector_store_type

        return results_df


def compare_vector_stores(
    pdf_paths: List[str],
    test_questions: List[Dict[str, Any]],
    store_types: List[str] = None
) -> pd.DataFrame:
    """
    –ü–æ—Ä—ñ–≤–Ω—é—î —Ä—ñ–∑–Ω—ñ –≤–µ–∫—Ç–æ—Ä–Ω—ñ —Å—Ö–æ–≤–∏—â–∞

    Args:
        pdf_paths: –°–ø–∏—Å–æ–∫ —à–ª—è—Ö—ñ–≤ –¥–æ PDF —Ñ–∞–π–ª—ñ–≤
        test_questions: –¢–µ—Å—Ç–æ–≤—ñ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è
        store_types: –¢–∏–ø–∏ –≤–µ–∫—Ç–æ—Ä–Ω–∏—Ö —Å—Ö–æ–≤–∏—â –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

    Returns:
        pd.DataFrame: –ó–≤–µ–¥–µ–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    """
    if store_types is None:
        store_types = ["chromadb", "faiss"]

    all_results = []

    for store_type in store_types:
        print(f"\n{'='*60}")
        print(f"–û–¶–Ü–ù–ö–ê: {store_type.upper()}")
        print(f"{'='*60}")

        evaluator = RAGEvaluator(vector_store_type=store_type)

        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–æ–∫—É–º–µ–Ω—Ç–∏
        evaluator.load_documents(pdf_paths)

        # –û—Ü—ñ–Ω—é—î–º–æ
        results = evaluator.evaluate_rag(test_questions)
        all_results.append(results)

    # –û–±'—î–¥–Ω—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    combined_results = pd.concat(all_results, ignore_index=True)

    return combined_results


def print_comparison_summary(results_df: pd.DataFrame):
    """
    –í–∏–≤–æ–¥–∏—Ç—å –∑–≤–µ–¥–µ–Ω—É —Ç–∞–±–ª–∏—Ü—é –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

    Args:
        results_df: DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ü—ñ–Ω–∫–∏
    """
    print("\n" + "="*60)
    print("–ó–í–ï–î–ï–ù–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø")
    print("="*60 + "\n")

    # –ì—Ä—É–ø—É—î–º–æ –ø–æ vector_store —Ç–∞ –æ–±—á–∏—Å–ª—é—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
    metric_columns = [col for col in results_df.columns
                     if col not in ['question', 'answer', 'contexts', 'ground_truth', 'vector_store']]

    summary = results_df.groupby('vector_store')[metric_columns].mean()

    print(summary.to_string())
    print("\n" + "="*60 + "\n")

    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø–µ—Ä–µ–º–æ–∂—Ü—è –ø–æ –∫–æ–∂–Ω—ñ–π –º–µ—Ç—Ä–∏—Ü—ñ
    print("üèÜ –ü–ï–†–ï–ú–û–ñ–¶–Ü –ü–û –ú–ï–¢–†–ò–ö–ê–ú:")
    for metric in metric_columns:
        winner = summary[metric].idxmax()
        winner_score = summary.loc[winner, metric]
        print(f"   {metric}: {winner} ({winner_score:.4f})")

    print("\n" + "="*60 + "\n")


# –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ñ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è
    test_questions = [
        {
            "question": "–©–æ —Ç–∞–∫–µ RAG?",
            "ground_truth": "RAG (Retrieval-Augmented Generation) - —Ü–µ —Ç–µ—Ö–Ω—ñ–∫–∞, —è–∫–∞ –ø–æ—î–¥–Ω—É—î –ø–æ—à—É–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—î—é –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π."
        },
        {
            "question": "–Ø–∫—ñ –ø–µ—Ä–µ–≤–∞–≥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–∏—Ö –±–∞–∑ –¥–∞–Ω–∏—Ö?",
            "ground_truth": "–í–µ–∫—Ç–æ—Ä–Ω—ñ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –¥–æ–∑–≤–æ–ª—è—é—Ç—å –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —à—É–∫–∞—Ç–∏ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ –ø–æ–¥—ñ–±–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏."
        },
        {
            "question": "–Ø–∫ –ø—Ä–∞—Ü—é—î —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫?",
            "ground_truth": "–°–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î embeddings –¥–ª—è –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ –ø–æ–¥—ñ–±–Ω–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º."
        }
    ]

    # –®–ª—è—Ö–∏ –¥–æ PDF —Ñ–∞–π–ª—ñ–≤
    pdf_paths = [
        # –î–æ–¥–∞–π—Ç–µ —à–ª—è—Ö–∏ –¥–æ –≤–∞—à–∏—Ö PDF —Ñ–∞–π–ª—ñ–≤
        # "path/to/document1.pdf",
        # "path/to/document2.pdf",
    ]

    if not pdf_paths:
        print("‚ö†Ô∏è  –î–æ–¥–∞–π—Ç–µ —à–ª—è—Ö–∏ –¥–æ PDF —Ñ–∞–π–ª—ñ–≤ —É –∑–º—ñ–Ω–Ω—É pdf_paths")
        print("   –ü—Ä–∏–∫–ª–∞–¥: pdf_paths = ['document.pdf']")
    else:
        # –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –≤–µ–∫—Ç–æ—Ä–Ω—ñ —Å—Ö–æ–≤–∏—â–∞
        results = compare_vector_stores(pdf_paths, test_questions)

        # –í–∏–≤–æ–¥–∏–º–æ –∑–≤–µ–¥–µ–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        print_comparison_summary(results)

        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–µ—Ç–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        results.to_csv("rag_evaluation_results.csv", index=False)
        print("‚úì –î–µ—Ç–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ rag_evaluation_results.csv")
