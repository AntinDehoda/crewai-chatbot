"""
Quick Comparison Tool - —à–≤–∏–¥–∫–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è ChromaDB vs FAISS
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Kubernetes –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—é –∑ –ø–∞–ø–∫–∏ data/pdf/
"""
import os
import time
from datetime import datetime
from typing import List, Dict
from pathlib import Path
import pandas as pd
from dotenv import load_dotenv

from utils import create_vector_store
from utils.pdf_processor import PDFProcessor

load_dotenv()


class VectorStoreComparator:
    """–ö–ª–∞—Å –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–∏—Ö —Å—Ö–æ–≤–∏—â"""

    def __init__(self):
        self.pdf_processor = PDFProcessor()
        self.results = []

    def benchmark_vector_store(
        self,
        store_type: str,
        pdf_paths: List[str],
        test_queries: List[str],
        k: int = 4
    ) -> Dict:
        """
        –¢–µ—Å—Ç—É—î –æ–¥–∏–Ω —Ç–∏–ø –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ö–æ–≤–∏—â–∞

        Args:
            store_type: –¢–∏–ø –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ö–æ–≤–∏—â–∞
            pdf_paths: –®–ª—è—Ö–∏ –¥–æ PDF —Ñ–∞–π–ª—ñ–≤
            test_queries: –¢–µ—Å—Ç–æ–≤—ñ –∑–∞–ø–∏—Ç–∏
            k: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤

        Returns:
            Dict: –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        """
        print(f"\n{'='*60}")
        print(f"BENCHMARK: {store_type.upper()}")
        print(f"{'='*60}\n")

        vector_store = create_vector_store(store_type)

        # 1. –í–∏–º—ñ—Ä—é—î–º–æ —á–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
        print("üìÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤...")
        load_start = time.time()

        total_chunks = 0
        for pdf_path in pdf_paths:
            if not os.path.exists(pdf_path):
                print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {pdf_path}")
                continue

            chunks = self.pdf_processor.load_pdf(pdf_path)
            vector_store.add_documents(chunks)
            total_chunks += len(chunks)
            print(f"   ‚úì {os.path.basename(pdf_path)}: {len(chunks)} chunk'—ñ–≤")

        load_time = time.time() - load_start
        print(f"\n   –ß–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {load_time:.3f}—Å")
        print(f"   –í—Å—å–æ–≥–æ chunk'—ñ–≤: {total_chunks}\n")

        # 2. –í–∏–º—ñ—Ä—é—î–º–æ —à–≤–∏–¥–∫—ñ—Å—Ç—å –ø–æ—à—É–∫—É
        print("üîç –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø–æ—à—É–∫—É...")
        search_times = []
        retrieved_docs = []

        for i, query in enumerate(test_queries, 1):
            print(f"   [{i}/{len(test_queries)}] {query[:50]}...")

            search_start = time.time()
            results = vector_store.search(query, k=k)
            search_time = time.time() - search_start

            search_times.append(search_time)
            retrieved_docs.append(len(results))

            print(f"      –ß–∞—Å: {search_time*1000:.2f}ms, –ó–Ω–∞–π–¥–µ–Ω–æ: {len(results)} –¥–æ–∫.")

        avg_search_time = sum(search_times) / len(search_times)
        print(f"\n   –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –ø–æ—à—É–∫—É: {avg_search_time*1000:.2f}ms\n")

        # 3. –¢–µ—Å—Ç –Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å (—è–∫—ñ—Å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞)
        print("üìä –ê–Ω–∞–ª—ñ–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ...")
        relevance_scores = []

        for query in test_queries[:3]:  # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à—ñ 3 –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
            results_with_scores = vector_store.search_with_scores(query, k=k)

            if results_with_scores:
                avg_score = sum(score for _, score in results_with_scores) / len(results_with_scores)
                relevance_scores.append(avg_score)

        avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0
        print(f"   –°–µ—Ä–µ–¥–Ω—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å: {avg_relevance:.4f}\n")

        # 4. –ü–∞–º'—è—Ç—å —Ç–∞ —Ä–æ–∑–º—ñ—Ä
        doc_count = vector_store.get_collection_count()
        print(f"   –î–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ: {doc_count}")

        # –ó–±–∏—Ä–∞—î–º–æ –º–µ—Ç—Ä–∏–∫–∏
        metrics = {
            "vector_store": store_type,
            "total_chunks": total_chunks,
            "load_time_sec": load_time,
            "avg_search_time_ms": avg_search_time * 1000,
            "avg_relevance_score": avg_relevance,
            "documents_count": doc_count,
            "chunks_per_second": total_chunks / load_time if load_time > 0 else 0
        }

        return metrics

    def compare_stores(
        self,
        pdf_paths: List[str],
        test_queries: List[str],
        store_types: List[str] = None
    ) -> pd.DataFrame:
        """
        –ü–æ—Ä—ñ–≤–Ω—é—î –≤–µ–∫—Ç–æ—Ä–Ω—ñ —Å—Ö–æ–≤–∏—â–∞

        Args:
            pdf_paths: –®–ª—è—Ö–∏ –¥–æ PDF —Ñ–∞–π–ª—ñ–≤
            test_queries: –¢–µ—Å—Ç–æ–≤—ñ –∑–∞–ø–∏—Ç–∏
            store_types: –¢–∏–ø–∏ –≤–µ–∫—Ç–æ—Ä–Ω–∏—Ö —Å—Ö–æ–≤–∏—â

        Returns:
            pd.DataFrame: –¢–∞–±–ª–∏—Ü—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        """
        if store_types is None:
            store_types = ["chromadb", "faiss"]

        results = []

        for store_type in store_types:
            metrics = self.benchmark_vector_store(store_type, pdf_paths, test_queries)
            results.append(metrics)

        return pd.DataFrame(results)


def print_comparison_table(df: pd.DataFrame):
    """–í–∏–≤–æ–¥–∏—Ç—å –∫—Ä–∞—Å–∏–≤—É —Ç–∞–±–ª–∏—Ü—é –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è"""

    print("\n" + "="*80)
    print(" " * 25 + "–ü–û–†–Ü–í–ù–Ø–õ–¨–ù–ê –¢–ê–ë–õ–ò–¶–Ø")
    print("="*80 + "\n")

    # –§–æ—Ä–º–∞—Ç—É—î–º–æ —Ç–∞–±–ª–∏—Ü—é
    comparison_data = []

    metrics = [
        ("–í–µ–∫—Ç–æ—Ä–Ω–µ —Å—Ö–æ–≤–∏—â–µ", "vector_store", "{}"),
        ("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ chunk'—ñ–≤", "total_chunks", "{:.0f}"),
        ("–ß–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (—Å)", "load_time_sec", "{:.3f}"),
        ("–®–≤–∏–¥–∫—ñ—Å—Ç—å –∑–∞–≤–∞–Ω—Ç. (chunk/s)", "chunks_per_second", "{:.1f}"),
        ("–°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –ø–æ—à—É–∫—É (ms)", "avg_search_time_ms", "{:.2f}"),
        ("–°–µ—Ä–µ–¥–Ω—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å", "avg_relevance_score", "{:.4f}"),
    ]

    for label, key, fmt in metrics:
        row = {"–ú–µ—Ç—Ä–∏–∫–∞": label}
        for _, store_data in df.iterrows():
            store_name = store_data["vector_store"]
            value = store_data[key]
            row[store_name] = fmt.format(value)
        comparison_data.append(row)

    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.to_string(index=False))

    print("\n" + "="*80 + "\n")

    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø–µ—Ä–µ–º–æ–∂—Ü—ñ–≤
    print("üèÜ –ü–ï–†–ï–ú–û–ñ–¶–Ü:\n")

    # –®–≤–∏–¥–∫—ñ—Å—Ç—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    fastest_load = df.loc[df['chunks_per_second'].idxmax()]
    print(f"   –ù–∞–π—à–≤–∏–¥—à–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {fastest_load['vector_store'].upper()}")
    print(f"      ({fastest_load['chunks_per_second']:.1f} chunk/s)\n")

    # –®–≤–∏–¥–∫—ñ—Å—Ç—å –ø–æ—à—É–∫—É
    fastest_search = df.loc[df['avg_search_time_ms'].idxmin()]
    print(f"   –ù–∞–π—à–≤–∏–¥—à–∏–π –ø–æ—à—É–∫: {fastest_search['vector_store'].upper()}")
    print(f"      ({fastest_search['avg_search_time_ms']:.2f} ms)\n")

    # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å
    best_relevance = df.loc[df['avg_relevance_score'].idxmax()]
    print(f"   –ù–∞–π–∫—Ä–∞—â–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å: {best_relevance['vector_store'].upper()}")
    print(f"      ({best_relevance['avg_relevance_score']:.4f})\n")

    print("="*80 + "\n")


def save_summary_to_txt(df: pd.DataFrame, output_folder: str = "test_results") -> str:
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–µ summary —É TXT —Ñ–∞–π–ª –∑ timestamp

    Args:
        df: DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        output_folder: –ü–∞–ø–∫–∞ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤

    Returns:
        str: –®–ª—è—Ö –¥–æ —Å—Ç–≤–æ—Ä–µ–Ω–æ–≥–æ —Ñ–∞–π–ª—É
    """
    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–∞–ø–∫—É —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
    os.makedirs(output_folder, exist_ok=True)

    # –ì–µ–Ω–µ—Ä—É—î–º–æ –Ω–∞–∑–≤—É —Ñ–∞–π–ª—É –∑ timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"vector_store_comparison_{timestamp}.txt"
    filepath = os.path.join(output_folder, filename)

    # –§–æ—Ä–º—É—î–º–æ —Ç–µ–∫—Å—Ç –∑–≤—ñ—Ç—É
    lines = []
    lines.append("="*80)
    lines.append(" "*15 + "QUICK PERFORMANCE BENCHMARK SUMMARY")
    lines.append("="*80)
    lines.append(f"\n–ß–∞—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å vector stores: {len(df)}")
    lines.append("\n" + "-"*80)
    lines.append("–ü–û–†–Ü–í–ù–Ø–õ–¨–ù–ê –¢–ê–ë–õ–ò–¶–Ø")
    lines.append("-"*80 + "\n")

    # –¢–∞–±–ª–∏—Ü—è –º–µ—Ç—Ä–∏–∫
    metrics = [
        ("–í–µ–∫—Ç–æ—Ä–Ω–µ —Å—Ö–æ–≤–∏—â–µ", "vector_store", "{}"),
        ("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ chunk'—ñ–≤", "total_chunks", "{:.0f}"),
        ("–ß–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (—Å)", "load_time_sec", "{:.3f}"),
        ("–®–≤–∏–¥–∫—ñ—Å—Ç—å –∑–∞–≤–∞–Ω—Ç. (chunk/s)", "chunks_per_second", "{:.1f}"),
        ("–°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –ø–æ—à—É–∫—É (ms)", "avg_search_time_ms", "{:.2f}"),
        ("–°–µ—Ä–µ–¥–Ω—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å", "avg_relevance_score", "{:.4f}"),
    ]

    # –í–∏–≤–æ–¥–∏–º–æ –¥–∞–Ω—ñ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ vector store
    for _, store_data in df.iterrows():
        store_name = store_data["vector_store"]
        lines.append(f"{store_name.upper()}:")
        for label, key, fmt in metrics[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–µ—Ä—à–∏–π (vector_store)
            value = store_data[key]
            lines.append(f"  ‚Ä¢ {label:30s}: {fmt.format(value)}")
        lines.append("")

    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø–µ—Ä–µ–º–æ–∂—Ü—ñ–≤
    lines.append("-"*80)
    lines.append("üèÜ –ü–ï–†–ï–ú–û–ñ–¶–Ü")
    lines.append("-"*80 + "\n")

    # –®–≤–∏–¥–∫—ñ—Å—Ç—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    fastest_load = df.loc[df['chunks_per_second'].idxmax()]
    slowest_load = df.loc[df['chunks_per_second'].idxmin()]
    speedup_load = fastest_load['chunks_per_second'] / slowest_load['chunks_per_second']

    lines.append("–ù–∞–π—à–≤–∏–¥—à–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è:")
    lines.append(f"  Winner: {fastest_load['vector_store'].upper()}")
    lines.append(f"  –®–≤–∏–¥–∫—ñ—Å—Ç—å: {fastest_load['chunks_per_second']:.1f} chunk/s")
    lines.append(f"  –ü–µ—Ä–µ–≤–∞–≥–∞: {speedup_load:.2f}x —à–≤–∏–¥—à–µ\n")

    # –®–≤–∏–¥–∫—ñ—Å—Ç—å –ø–æ—à—É–∫—É
    fastest_search = df.loc[df['avg_search_time_ms'].idxmin()]
    slowest_search = df.loc[df['avg_search_time_ms'].idxmax()]
    speedup_search = slowest_search['avg_search_time_ms'] / fastest_search['avg_search_time_ms']

    lines.append("–ù–∞–π—à–≤–∏–¥—à–∏–π –ø–æ—à—É–∫:")
    lines.append(f"  Winner: {fastest_search['vector_store'].upper()}")
    lines.append(f"  –ß–∞—Å –ø–æ—à—É–∫—É: {fastest_search['avg_search_time_ms']:.2f} ms")
    lines.append(f"  –ü–µ—Ä–µ–≤–∞–≥–∞: {speedup_search:.2f}x —à–≤–∏–¥—à–µ\n")

    # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å
    best_relevance = df.loc[df['avg_relevance_score'].idxmax()]
    worst_relevance = df.loc[df['avg_relevance_score'].idxmin()]
    diff_relevance = best_relevance['avg_relevance_score'] - worst_relevance['avg_relevance_score']
    diff_percent = (diff_relevance / worst_relevance['avg_relevance_score'] * 100) if worst_relevance['avg_relevance_score'] > 0 else 0

    lines.append("–ù–∞–π–∫—Ä–∞—â–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å:")
    lines.append(f"  Winner: {best_relevance['vector_store'].upper()}")
    lines.append(f"  Score: {best_relevance['avg_relevance_score']:.4f}")
    lines.append(f"  –ü–µ—Ä–µ–≤–∞–≥–∞: +{diff_relevance:.4f} (+{diff_percent:.1f}%)\n")

    # –í–∏—Å–Ω–æ–≤–∫–∏
    lines.append("-"*80)
    lines.append("–í–ò–°–ù–û–í–ö–ò")
    lines.append("-"*80 + "\n")

    # –†–∞—Ö—É—î–º–æ –ø–µ—Ä–µ–º–æ–∂—Ü—ñ–≤
    wins = {}
    for _, store_data in df.iterrows():
        store = store_data['vector_store']
        wins[store] = 0

    wins[fastest_load['vector_store']] += 1
    wins[fastest_search['vector_store']] += 1
    wins[best_relevance['vector_store']] += 1

    for store in wins:
        lines.append(f"{store.upper()}:")
        lines.append(f"  –í–∏–≥—Ä–∞–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π: {wins[store]}/3")

    lines.append("\n" + "="*80)
    lines.append("–û–ü–ò–° –ú–ï–¢–†–ò–ö:")
    lines.append("="*80)
    lines.append("‚Ä¢ –®–≤–∏–¥–∫—ñ—Å—Ç—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è - –°–∫—ñ–ª—å–∫–∏ chunk'—ñ–≤ –Ω–∞ —Å–µ–∫—É–Ω–¥—É –º–æ–∂–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏")
    lines.append("‚Ä¢ –®–≤–∏–¥–∫—ñ—Å—Ç—å –ø–æ—à—É–∫—É       - –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –ø–æ—à—É–∫—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
    lines.append("‚Ä¢ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å          - –Ø–∫—ñ—Å—Ç—å –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ (similarity score)")
    lines.append("="*80 + "\n")

    # –ó–∞–ø–∏—Å—É—î–º–æ —É —Ñ–∞–π–ª
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))

    print(f"‚úì –ê–Ω–∞–ª—ñ—Ç–∏—á–Ω–µ summary –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {filepath}")

    return filepath


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""

    # Kubernetes —Ç–µ—Å—Ç–æ–≤—ñ –∑–∞–ø–∏—Ç–∏
    test_queries = [
        "What is Kubernetes?",
        "What is a Pod in Kubernetes?",
        "What is the difference between a Pod and a Container?",
        "What is a Deployment in Kubernetes?",
        "What is a Service in Kubernetes?",
        "What are the types of Kubernetes Services?",
        "What is an Ingress?",
        "How do you perform a rolling update?",
        "What are the best practices for managing secrets in Kubernetes?",
        "How do you implement auto-scaling in Kubernetes?",
    ]

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤—Å—ñ PDF –∑ –ø–∞–ø–∫–∏ data/pdf/
    pdf_folder = Path("data/pdf")
    if not pdf_folder.exists():
        print("\n" + "="*60)
        print("‚ö†Ô∏è  –ü–ê–ü–ö–ê data/pdf/ –ù–ï –ó–ù–ê–ô–î–ï–ù–ê")
        print("="*60)
        print("\n–°—Ç–≤–æ—Ä—ñ—Ç—å –ø–∞–ø–∫—É data/pdf/ —Ç–∞ –¥–æ–¥–∞–π—Ç–µ —Ç—É–¥–∏ Kubernetes PDF –¥–æ–∫—É–º–µ–Ω—Ç–∏")
        print("\n–ü—Ä–∏–∫–ª–∞–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏:")
        print("   data/pdf/")
        print("       ‚îú‚îÄ‚îÄ kubernetes-basics.pdf")
        print("       ‚îú‚îÄ‚îÄ kubernetes-networking.pdf")
        print("       ‚îî‚îÄ‚îÄ kubernetes-storage.pdf\n")
        return

    pdf_paths = list(pdf_folder.glob("*.pdf"))

    if not pdf_paths:
        print("\n" + "="*60)
        print("‚ö†Ô∏è  PDF –§–ê–ô–õ–ò –ù–ï –ó–ù–ê–ô–î–ï–ù–û")
        print("="*60)
        print("\n–î–æ–¥–∞–π—Ç–µ Kubernetes PDF –¥–æ–∫—É–º–µ–Ω—Ç–∏ –≤ –ø–∞–ø–∫—É data/pdf/")
        print("\n–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞:")
        print("   - Official Kubernetes documentation exports")
        print("   - Kubernetes in Action (book)")
        print("   - Kubernetes patterns documentation\n")
        return

    print(f"\nüìö –ó–Ω–∞–π–¥–µ–Ω–æ {len(pdf_paths)} PDF —Ñ–∞–π–ª(—ñ–≤) –≤ data/pdf/:")
    for pdf_path in pdf_paths:
        print(f"   ‚Ä¢ {pdf_path.name}")
    print()

    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ Path –æ–±'—î–∫—Ç–∏ –≤ —Ä—è–¥–∫–∏
    pdf_paths = [str(p) for p in pdf_paths]

    # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–º–ø–∞—Ä–∞—Ç–æ—Ä
    comparator = VectorStoreComparator()

    # –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –≤–µ–∫—Ç–æ—Ä–Ω—ñ —Å—Ö–æ–≤–∏—â–∞
    results_df = comparator.compare_stores(pdf_paths, test_queries)

    # –í–∏–≤–æ–¥–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    print_comparison_table(results_df)

    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–µ summary —É TXT –∑ timestamp
    save_summary_to_txt(results_df)


if __name__ == "__main__":
    main()
